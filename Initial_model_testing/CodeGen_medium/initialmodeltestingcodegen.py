# -*- coding: utf-8 -*-
"""InitialModelTestingCodeGen

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L3cblyyT-QIHtUJiI8vofPo7iJ5Snjba
"""

!pip install transformers datasets

import json
from transformers import AutoModelForCausalLM, AutoTokenizer

file_path = "/content/Testing_dataset_simple.jsonl"

with open(file_path, 'r') as f:
    data = [json.loads(line) for line in f]

print("Example Question:")
print(data[0]["question"])

instruction = "Convert this word problem into Python code to calculate the answer:"
for item in data:
    item["question"] = f"{instruction} {item['question']}"

print("Example Question with Instruction:")
print(data[0]["question"])

model_name = "Salesforce/codegen-2B-mono"  #switch between models
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def generate_code_codegen(prompt, model, tokenizer, max_length=100):
    inputs = tokenizer(prompt, return_tensors="pt")

    outputs = model.generate(
        inputs["input_ids"],
        max_length=max_length,
        num_return_sequences=1,
        temperature=0.7,
        top_p=0.95,
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

question = data[0]["question"]
generated_code = generate_code_codegen(question, model, tokenizer)
print("Question:", question)
print("Generated Code:", generated_code)

for item in data[:10]:  #Limits to the first 10 questions
    question = item["question"]
    print("Question:", question)
    generated_code = generate_code_codegen(question, model, tokenizer)
    print("Generated Code:", generated_code)
    print("-" * 50)