# -*- coding: utf-8 -*-
"""InitialModelTestingGPT2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12O91hs9kZ4R_42AXMCDJnkhjrB7RvKQ5
"""

!pip install transformers datasets

import json
from transformers import GPT2LMHeadModel, GPT2Tokenizer

file_path = "/content/Testing_dataset_simple.jsonl"

with open(file_path, 'r') as f:
    data = [json.loads(line) for line in f]

print("Example Question:")
print(data[0]["question"])

instruction = "Convert this word problem into Python code to calculate the answer:"
for item in data:
    item["question"] = f"{instruction} {item['question']}"

print("Example Question with Instruction:")
print(data[0]["question"])

model_name = "gpt2-medium"  #switch between models
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

def generate_code(prompt, model, tokenizer, max_length=100):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(inputs["input_ids"], max_length=max_length, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

question = data[0]["question"]
generated_code = generate_code(question, model, tokenizer)
print("Question:", question)
print("Generated Code:", generated_code)

for item in data[:10]:  #Limits to the first 10 questions for it not to take forever
    question = item["question"]
    print("Question:", question)
    generated_code = generate_code(question, model, tokenizer)
    print("Generated Code:", generated_code)
    print("-" * 50)